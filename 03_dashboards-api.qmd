# Dashboards and APIs

## Learning Objectives

By the end of today's session you should be able to:

1.  Understand the basic principles of APIs
2.  Download and visualise data from NOMIS using the NOMIS API
3.  Understand the basic principles of flexdashboard
4.  Build a basic dashboard using flexdashboard

```{r include = FALSE}
## Today's packages
library(tidyverse)
library(sf)
library(nomisr)
library(flexdashboard)
```

## Introduction to APIs

Web services make their data easily accessible to computer programs like R through use of an Application Programming Interface (API). Today's practical will teach you how to access data from APIs, and load them into your R environment for analysis.

To download data from an API you need to send a HTTP request to a server, which tells the server to return the specific parcel of data that matches the criteria in the HTTP request.

For example, on NOMIS there is a page called ['Census 2021 Bulk Data Download'](https://www.nomisweb.co.uk/sources/census_2021_bulk), which contains .zip files for different tables of data available from the latest census.

Now you should go to the ['Census 2021 Bulk Data Download'](https://www.nomisweb.co.uk/sources/census_2021_bulk) page, and see what it contains.

![NOMIS](figs/04/nomis.png)

There are lots of files on the web page - e.g. `census2021-ts001.zip`, `census2021-ts007a.zip`.

You can click on these files individually, download them to your PC, unzip them and read them into R. Alternatively, we can programmatically download the data directly from the webpage. If you 'right click' on one of the .zip files and press 'copy link', you will have a URL which can access that specific .zip file, as below:

```{r}
url <- "https://www.nomisweb.co.uk/output/census/2021/census2021-ts061.zip"
url
```

The specific URL above relates to table [TS061 - "Method of Travel to Work"](https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/employmentandemployeetypes/bulletins/traveltoworkenglandandwales/census2021), which is the same dataset we have been using throughout this course.

Now I'm going to show you how to download the .zip file, and read in the file of data we used yesterday. This is a really basic example of using an API, which shows how you can download data from NOMIS into your environment, without having to physically go and download it, save it to a folder, unzip it and read it into memory.

First, let's download the .zip file - this line of code downloads the .zip file to your local machine. It creates a new file in your working directory called 'temp.zip' - go and take a look!

```{r}
## Download the .zip file, using the url set above
download.file(url, "temp.zip")
```

Next we need to unzip the folder, to get to the datasets stored within:

```{r}
## First set where you want the unzipped files to be stored
outDir <- "data/unzip"
## Unzip the folder to the data/unzip folder
unzip("temp.zip", exdir = outDir)
```

Ok so now that you've downloaded the files to your local machine, we can look and see what files are available to us:

```{r}
## Use list.files() to see what we unzipped
list.files("data/unzip")
```

Thankfully, NOMIS use a really standard naming protocol for their files, which makes it really easy to tell what each of the files contains. If you cast your mind back to yesterday, we used a file called "census2021-ts061-lsoa.csv", which we provided to you as part of the course materials. However, as you can see from the code above, you have now programmatically downloaded the same file, which we can read in:

```{r}
## Read in the LSOA census data
db <- read.csv("data/unzip/census2021-ts061-lsoa.csv")
```

### Independent exercise - Over to you! (15 - 20 mins)

1.  As a recap, see if you can reproduce one of the visualisations we produced yesterday using the data we have just scraped from the API.
2.  Test downloading two more datasets from NOMIS, by swapping in new URLs, and reading in one of the files from the folder you download.
3.  (*optional*) Produce an interesting visualisation from that new dataset.

**Solution 2**

```{r}
## Set the new URL
url2 <- "https://www.nomisweb.co.uk/output/census/2021/census2021-ts066.zip"

## Download the files
download.file(url2, "temp2.zip")

## Unzip the files
outDir2 <- "data/unzip2"
## Unzip the folder to the data/unzip folder
unzip("temp2.zip", exdir = outDir2)

## Read in the LSOA census data
db2 <- read.csv("data/unzip2/census2021-ts066-lsoa.csv")

## Select some columns to work with, and calculate % student
db2_clean <- db2 %>%
  select(geography.code, Economic.activity.status..Total..All.usual.residents.aged.16.years.and.over, Economic.activity.status..Economically.inactive..Student) %>%
  setNames(c("LSOA21CD", "total", "student")) %>%
  mutate(student = (student / total) * 100)
```

**Solution 3**

```{r}
## Produce a histogram
ggplot(data = db2_clean, aes(x = student)) +
  geom_density(fill = "orange") +
  labs(x = "Population who are students (%)", y = "Number of LSOAs",
       caption = "Data: UK Census (2021) - 'Economic Activity Status' (ts066)") +
  theme_minimal()
```

So what have we achieved?

1.  You can now programmatically download datasets from the NOMIS bulk census page, without needing to download the files.
2.  You can produce visualisations using these different datasets.
3.  You are in a position to automate download and visualisation of census data from NOMIS.

### Advanced Bulk Downloading

But what if you want to automate this download in a much quicker manner. The NOMIS API is a good way of doing this (and we'll be discussing this shortly), but the other way you can do this is through the use of functions.

Functions are "self contained modules of code that accomplish a specific task". They normally take data in soem form, perform a number of modifications to it, and then return some form of result.

The basic syntax of a function is as follows:

```{r}
## Specify a new function
function1 <- function(x) {
  
}
```

There are three important things to notice here:

1.  Function name (e.g., `function1`) - enables you to call the function at a later point.
2.  Object (e.g., `function(x)`) - is the input data or parameter that you are going to be using.
3.  Steps (e.g., `{}`) - anything within the curly brackets will be a series of steps that are applied.

So a basic example is we can ask the function to return the first five rows of x:

```{r}
## Specify a new function - first five
firstfive <- function(x) {
  head(x)
}
```

Now you need to run the function on an object - let's do it on our census dataset:

```{r}
## Apply the function
firstfive(db)
```

So, that's a *really* basic example of how to use a function.

Let me show you how functions can be used to clean datasets programmatically:

```{r}
## Function cleans the raw census dataset
cleandb <- function(x) {
  
  ## Select columns
  out <- x %>%
    select(geography.code, 
           Method.of.travel.to.workplace..Total..All.usual.residents.aged.16.years.and.over.in.employment.the.week.before.the.census,
           Method.of.travel.to.workplace..Train) %>%
    setNames(c("LSOA21CD", "total", "train")) %>%
    mutate(pctTrain = (train / total) * 100)
  
  ## Return the new dataframe
  return(out)
  
}
```

Now that the function has been specified, you can pass your dataset through it:

```{r}
## Pass census data through the new function
cleaned <- cleandb(db)

## Look at it
head(cleaned)
```

There are lots of reasons why this approach is useful:

1.  Can provide a standard set of code chunks to clean data from NOMIS, and other sources
2.  Can be looped across multiple different datasets
3.  Can be customised to perform different steps of cleaning that you need for your use case.

However, perhaps the most useful thing functions can do, is be used to apply an API query and return a cleaned dataset for your use.

Let's have a look at an example: [`tidylodes`](https://github.com/patrickballantyne/tidylodes).

![tidylodes](figs/04/tidylodes.png)

We can implement some of these techniques and ideas to automate scraping of data from NOMIS.

For example, take the URL we set above:

```{r}
url
```

If we wrote a function that could enable specification of a different table number, we could build a pipeline where you only have to change a couple of characters to return a cleaned dataset for your reports.

For example, the other URL I provided in my solution above:

```{r}
url2
```

You'll notice the only difference with this URL is the `ts066` string at the end of the URL. Let's start building a function that can swap these different characters in and out:

```{r}
downloadNOMIS <- function(x = "ts061") {
  
  ## Create the URL, based on the character supplied in x
  url <- paste0("https://www.nomisweb.co.uk/output/census/2021/census2021-", x, ".zip")
  return(url)
}
```

Let's test this:

```{r}
downloadNOMIS("ts061")
```

And then with a different table number

```{r}
downloadNOMIS("ts066")
```

Ok, so now we need to add to the function, to get R to download from the URL:

```{r}
downloadNOMIS <- function(x = "ts061") {
  
  ## Create the URL, based on the character supplied in x
  url <- paste0("https://www.nomisweb.co.uk/output/census/2021/census2021-", x, ".zip")
  
  ## Download
  download.file(url, "table.zip")
  
}
```

Test it:

```{r}
downloadNOMIS("ts066")
```

Now if you go into your working directory, you should have a .zip folder called `table`.

Final steps: unzipping and checking all the files are there:

```{r}
downloadNOMIS <- function(x = "ts061") {
  
  ## Create the URL, based on the character supplied in x
  url <- paste0("https://www.nomisweb.co.uk/output/census/2021/census2021-", x, ".zip")
  
  ## Download
  download.file(url, "table.zip")
  
  ## Unzip the files
  outDir2 <- "data/tableUZ"
  ## Unzip the folder to the data/unzip folder
  unzip("table.zip", exdir = outDir2)

}
```

Now run the command for a table of your choosing:

```{r}
downloadNOMIS("ts004")
```

You can check this has worked by looking at the files available in the `tableUZ` folder, set up in the function above:

```{r}
list.files("data/tableUZ")
```

Awesome! You have created a function that enables you to have access to tables of data from NOMIS very easily. Finally, you might be interested in asking the function to read in a specific file - you can do this easily from NOMIS as the file naming protocols are very organised.

```{r}
downloadNOMIS <- function(x = "ts061") {
  
  ## Create the URL, based on the character supplied in x
  url <- paste0("https://www.nomisweb.co.uk/output/census/2021/census2021-", x, ".zip")
  
  ## Download
  download.file(url, "table.zip")
  
  ## Unzip the files
  outDir2 <- "data/tableUZ"
  ## Unzip the folder to the data/unzip folder
  unzip("table.zip", exdir = outDir2)
  
  ## Read in the regional level data
  db <- read.csv(paste0("data/tableUZ/census2021-", x, "-rgn.csv"))
  return(db)
}
```

Test it

```{r}
## Download and read in
t <- downloadNOMIS("ts003")

## Inspect
head(t)
```

The final thing I want to show you before we move off of functions is how to apply functions sequentially. Functions can be applied to lists really easily using the `lapply()` command. This means you could supply multiple names of tables that you want, and get R to download, read and clean them for you using the techniques I have just shown you.

For example, take a list like this:

```{r}
ls <- c("ts001", "ts002", "ts003")
ls
```

We can apply the `downloadNOMIS` function to these three tables, to read in the regional-level tables. The way to do this is to use the `lapply` command, which takes as arguments 1) a list and 2) a function to apply over the list.

In our case this would be:

```{r}
## Download three tables from NOMIS
tables <- lapply(ls, downloadNOMIS)
```

Once this has finished running, you are left with a list of tables, see:

```{r}
str(tables)
```

You can select one out individually:

```{r}
## Select the first table
table1 <- tables[[1]]
head(table1)
```

Or because they all have the `geography.code`, `geography` and `date` column, you can just attach them into one big table. This relies on the use of the `purrr` package, which is full of useful tools for working with lists.

```{r}
## Bind together
test <- tables %>%
  reduce(left_join, by = c("date", "geography", "geography.code"))

## Have a look
colnames(test)
```

## Using the NOMIS API

One of the things that you see more commonly in practice is the construction of specific R packages used to access APIs, with supporting documentation and specific functions that make it easier to use the API.

One such example is [`nomisr`](https://docs.evanodell.com/nomisr/articles/introduction.html), which is an R package that was built to enable users to query data from NOMIS. It is free to access and contains up-to-date official statistics including data from the latest Census, Labour Force Survey and DWP benefit statistics.

In the section that follows, I'm going to be showing you how to use the nomisr package to download datasets.

Vast amounts of data are available through NOMIS, so you need to use some of the different functions within `nomisr` to identify the specific datasets you want to use. An example is presented below which searches for datasets within NOMIS that are specifically about 'Travel':

```{r}
## Search for data on Labour Force
search <- nomis_search("*Travel*")
```

This returns a dataframe (which you should see in your environment) that describes all of the different NOMIS held datasets where 'Travel' is mentioned. The column perhaps of most interest is the short name for the different datasets, which you can inspect below:

```{r}
## Have a look at the first six datasets 
head(search$name.value)
```

If you open up the dataframe in your environment and scroll down you should see one row has the value - `TS061 - Method used to travel to work` - which is the one we've been using a lot in this practical.

We can filter to this row very easily using the `filter()` command that we introduced yesterday:

```{r}
## Filter to row of interest
search_sub <- search %>%
  filter(name.value == "TS061 - Method used to travel to work")

## Have a look at the result
search_sub
```

Notice how the table ID is `NM_2078_1`. We can get some metadata for this dataset very easily using the nomis_get_metadata() command. First, let's see what measures are available:

```{r}
## Supply the ID of the row we're interested in, and the second parameters specifies we'd like to know more about the measures
nomis_get_metadata(search_sub$id, "measures")
```

So for TS061, we can get both raw counts ('value') and percent. Notice how the ID for counts is `20100` and the ID for percent is `20301`. Let's now see what geographies are available:

```{r}
## Supply the ID of the row we're interested in, and the second parameter specifies that we want to know more about geographies
nomis_get_metadata(search_sub$id, "geography")
```

Ok, so this is telling us the different geographic levels we can download the data for. However, if we add an additional parameter to this, we can also see the specific geographic units that this data is available at:

```{r}
## Add in an additional parameter
nomis_get_metadata(search_sub$id, "geography", "TYPE")
```

So there are a variety of different geographic levels at which we can download the dataset, including LSOA - see `2021 super output areas - lower layer`. Notice how the ID for LSOAs is `TYPE151`.

Those steps we have just performed basically give us everything we need to download the dataset directly from the NOMIS API using the package, instead of downloading the .zip files directly. Let's download the file - it could take a while! If you don't understand any of the specific inputs to this line of code, feel free to shout Patrick to talk it through.

```{r}
## Download the file
db_v2 <- nomis_get_data(id = "NM_2078_1", time = "latest", geography = c("TYPE151"), measures = "20301")
```

The format the data is presented in is not the most intuitive, so those reshaping skills we acquired yesterday are going to come in handy here again!

Firstly, let's get the columns we need for our analysis - LSOA codes, the different modes of transport and the actual reported values.

```{r}
## Select columns of interest
db_clean <- db_v2 %>%
  select(GEOGRAPHY_CODE, C2021_TTWMETH_12_NAME, OBS_VALUE) 

## Inspect
head(db_clean)
```

So as you can see from the table, it's actually in a long format, whereas we might want it to be in a wide format, where each column is the % of people using each transport mode. Let's use the `pivot_wider()` command to change this:

```{r}
## Go from long to wide
db_clean <- db_clean %>%
  pivot_wider(names_from = C2021_TTWMETH_12_NAME, values_from = OBS_VALUE)

## Inspect
head(db_clean)
```

Great, that's worked! You'll also notice the number of rows of db_clean matches that of db (which was the file we unzipped at the start of the practical).

Some final data cleaning steps:

```{r}
## Tidy up the dataset
db_final <- db_clean %>%
  setNames(c("LSOA21CD", "total", "work_from_home", "underground_metro", "train", "bus_minibus_coach", 
             "taxi", "motorcycle", "car_driving", "car_passenger", "bicycle", "foot", "other")) ## set new names
```

And then we can easily produce one of the visualisations from yesterday:

```{r}
## Reproduce the scatter plot from yesterday's class
ggplot(data = db_final, aes(x = work_from_home, y = car_driving)) +
  geom_point(alpha = 0.3, size = 0.35) +
  geom_smooth(method = "lm") +
  xlim(0, 100) +
  ylim(0, 100) +
  labs(x = "Population who work from home (%)", y = "Population who drive to work (%)",
       caption = "Data: UK Census (2021) - 'Method of travel to work' (ts061)") +
  theme_minimal()
```

Nice! You have now learned how to programmatically use the API to scrape data from NOMIS, bypassing the need to download and unzip the files directly. Now, some independent tasks to check you can reproduce the steps.

### Independent exercise - Over to you!

1.  Experiment with downloading a different dataset using the `nomisr` package, and clean it.
2.  Produce an interesting visualisation using your chosen dataset.
3.  (*optional*) See if you can attach the LAD names to your dataset, and produce a visualisation that examines LAD differences in your chosen dataset - recommend you choose a dataset at either LSOA or MSOA geography to use yesterday's lookup table. If you are unsure of how to do this, you will need to go back to `Day 2 - Data Visualisation`.

## Building Dashboards in R

Dashboards are often a great way to share results and analyses with others. There are a number of ways you can build dashboards in R, including:

1.  Using markdown (`flexdashboard` R package)
2.  Using `R shiny`.

The former offers you to create a dashboard with panels and pages very easily, and has significant advantages over R Shiny:

1.  Minimal coding required.
2.  Dashboard can be distributed as the .html file, with no server required.
3.  Other packages can hook into the dashboard to add interactivity.

### Getting started

To build a dashboard using R markdown, we will need to use an alternate type of computational workbook - thus far we have been working with Quarto files (.qmd), but now we need to switch to the format that supports "Flex Dashboards".

However, `flexdashboard` runs into problems when linked to an existing R project, especially one which is hooked up to Quarto.

**Important** So for this part of the project please create a new directory where you want to build your dashboard. For example, I've created a new folder called 'Dashboard', which is where I will be building my dashboard. Also, make sure to copy some of the datasets we have been using into this new directory:

![New Directory](figs/04/directory.png)

The final step is to create a new file which will be used to build your dashboard. In R, Go to **File** \> **New File** \> **R Markdown** \> **From Template** \> **Flex Dashboard** (see below).

![New File](figs/04/file.png)

The file should open up automatically, and should look the one below:

![Blank Dashboard Template](figs/04/dashboard-raw.png)

Now save it **inside your new folder** as something you can remember - e.g., dashboard.Rmd. It is vital that this new .Rmd is saved within your new folder, which should now look like this:

![Full Directory](figs/04/complete.png)

When you open up the .Rmd file, you should be able to see the 'Knit' button as below. If you instead see the 'Render' button, you've not saved the .rmd in your new directory. Speak to Patrick if you run into problems here.

![Knit Button](figs/04/knit.png)

### Introduction to Flex Dashboard

For the remainder of the practical, you need to be working in your new .Rmd file. This is where you will build and deploy your dashboard, so please make sure you are working in this file - 'dashboard.Rmd'.

Inside the file you will notice a couple of different things:

1.  Code blocks - you will see code blocks like those you have been running in this document, which can be used to run lines of code easily.
2.  YAML header - at the top of the new file is a YAML header, which is where you can set up the basic metadata for the dashboard.

Have a go at changing your YAML header to the following:

![YAML](figs/04/yaml.png)

These parameters are doing the following:

-   `title`: sets a title to appear at the top of the dashboard.
-   `orientation`: determines whether charts should be aligned in rows or columns
-   `vertical_layout`: sets the dashboard to fill available browser height.

Press 'Knit' and see what happens...

R should open up a new window that looks like this:

![First Dashboard](figs/04/first.png)

So you can see that the dashboard has three panes set up to host different types of visualisation. In the .Rmd file you'll notice that each `##` denotes the start of a new column, and each `###` denotes a new pane for visualisation.

Let's change the layout slightly, to create a grid of four equally sized panes. To do this you need to:

1.  Change the data-width parameter to be equal for both columns
2.  Add a second panel under column one

Here is what the code looks like:

![Code](figs/04/resize-code.png)

Now press 'Knit' again, and see what has changed:

![Resized](figs/04/resize-out.png)

Cool! So the dashboard layout is set up and ready to go!

### Independent exercise - Over to you!

1.  Swap the orientation to be columns
2.  See if you can set up the dashboard to be structured as 1 large panel on the left, and three on the right.
3.  (*optional*) Change the section headers for each panel to list some potential visualisations you might put in each, based on yesterday's class.

SOLUTION:

![Solution](figs/04/solution1.png)

### Embedding visualisations

When building a dashboard, a key component will be adding in a number of visualisations to enhance the information being conveyed by the dashboard. As you have seen so far, `flexdashboard` creates panes to host plots and visualisations on.

However, the visualisations need to be produced within the .Rmd file before they can be displayed on the dashboard itself. This is where you can use code chunks within the .Rmd file to re-run some of the analyses we did yesterday to produce visuals for the dashboard.

The code chunks need to be placed before the dashboard structural parameters - i.e. before the `##` Column commands in the script. Have a look at the example below where we read in and clean the census data we were using on Day 2:

![How to insert code](figs/04/code-example.png)

Then, once you have the read the data in you can produce one of the visualisations we made yesterday - such as the scatter plot we made yesterday:

![Make a plot](figs/04/code-example2.png)

Now the final step is to think about embedding this plot so it can be displayed on the plot. To do this, all you need to do is call the plot in one of the code blocks which are set up to host visualisations, see below:

![Embed a plot](figs/04/code-example3.png)

Now hit 'Knit' and you should see this plot on the dashboard:

![First stage](figs/04/dashboard-p1.png)

### Independent exercise - Over to you!! (30 - 45 mins)

Have a go at embedding some more visualisations on the dashboard. To do this you will need to reproduce lots of the analysis you did on Day 2.

Below I've included something that I put together, which is something you could aim to try and generate. Have a look at [this vignette](https://rstudio.github.io/flexdashboard/articles/using.html) produced by the `flexdashboard` package developers for some ideas of different elements and interactive functionalities you might want to build into your dashboard. 

![Transport Dashboard](figs/04/final-dashboard.png)

## Other Useful Reporting Techniques

We have shown you the power of using R and Quarto for generating computational workbooks where you can view both the figures/outputs and code used to generate them, all at once.

### Constructing Reports

The .html files that render alongside these .qmd files can be seen as 'reports' in a way. However, there is a whole host of design-related modifications you can make to the .qmd file which will enhance the appearance of the output report.

There is lots of useful information online about creating nice reports using R markdown - including this one from [The Epidemiologist R Handbook](https://epirhandbook.com/en/reports-with-r-markdown.html).

### Routine Reporting

The Epidemiologist Handbook also has a really nice section on how to use R to run reports routinely, using the `reportfactory` R package. Have a a look at the [Organising routing reports](https://epirhandbook.com/en/organizing-routine-reports.html) guide if you are interested in learning more about this!
